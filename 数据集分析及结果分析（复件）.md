# 数据集分析

本项目使用的数据集为项目团队自主拍摄采集。使用了十余种编码的ARUCO作为目标，拍摄背景为卫星等比模型，环境包括理想光照、过曝、几乎无光照等多种情况。并增加了局部遮挡目标，小目标的数量，以提升对局部遮挡和小目标的泛化能力。经过数据清洗以及labelimg标注，取得数据集共4125张图像，按照8:2的比例将数据集划分为训练集和测试集，其中训练集3300张，测试集825张。

其一，对目标中心点分布进行可视化分析，结果如下图所示，横纵坐标表示目标中心点在图像中的位置，可见数据集目标在图像中位置分布较为均匀：

<img src="./../photo/image-20240228190016243.png" alt="image-20240228190016243" style="zoom:50%;" />

其二，对目标大小进行可视化分析，结果如下图所示，横坐标表示目标的宽、纵坐标表示目标的高：

<img src="./../photo/image-20240228202434997.png" alt="image-20240228202434997" style="zoom:50%;" />

# 网络改进

1.注意力机制

> 引入了SE注意力机制，论文原文Squeeze-and-Excitation Networks：https://arxiv.org/abs/1709.01507

2.BiFPN加权双向特征金字塔

> 论文原文EfficientDet: Scalable and Efficient Object Detection：https://arxiv.org/abs/1911.09070

3.改进后的网络结构

> 序号      from  n    params             module                                  arguments                     
>   0                -1  1      5280                Conv                      [3, 48, 6, 2, 2]              
>   1                -1  1     41664               Conv                      [48, 96, 3, 2]                
>   2                -1  2     65280                   C3                        [96, 96, 2]                   
>   3                -1  1      1152                   SE                        [96, 96]                      
>   4                -1  1    166272               Conv                      [96, 192, 3, 2]               
>   5                -1  4    444672                 C3                        [192, 192, 4]                 
>   6                -1  1      4608                   SE                        [192, 192]                    
>   7                -1  1    664320             Conv                      [192, 384, 3, 2]              
>   8                -1  6   2512896                C3                        [384, 384, 6]                 
>   9                -1  1     18432                   SE                        [384, 384]                    
>  10                -1  1   2655744                  Conv                      [384, 768, 3, 2]              
>  11                -1  2   4134912                   C3                        [768, 768, 2]                 
>  12                -1  1     73728                   SE                        [768, 768]                    
>  13                -1  1   1476864                   SPPF                      [768, 768, 5]                 
>  14                -1  1    295680                   Conv                      [768, 384, 1, 1]              
>  15                -1  1         0                   Upsample                     [None, 2, 'nearest']          
>  16           [-1, 8]  1         2                   BiFPN_Concat2             [1]                           
>  17                -1  2   1182720                   C3                        [768, 384, 2, False]          
>  18                -1  1     74112                   Conv                      [384, 192, 1, 1]              
>  19                -1  1         0                   Upsample                     [None, 2, 'nearest']          
>  20           [-1, 5]  1         2                   BiFPN_Concat2             [1]                           
>  21                -1  2    296448                   C3                        [384, 192, 2, False]          
>  22                -1  1    332160                   Conv                      [192, 192, 3, 2]              
>  23       [-1, 18, 9]  1         3                   BiFPN_Concat3             [1]                           
>  24                -1  2   1182720                   C3                        [768, 384, 2, False]          
>  25                -1  1   1327872                   Conv                      [384, 384, 3, 2]              
>  26          [-1, 14]  1         2                   BiFPN_Concat2             [1]                           
>  27                -1  2   4134912                   C3                        [768, 768, 2, False]          

from表示输入来自哪一层，其中-1表示输入来自上一层，params表示参数量的大小，module表示子模块的名称，arguements表示模块的参数信息，其中包含输入输出通道数量，卷积核尺寸大小等信息。

# 实验结果

1.实验环境

>实验操作系统为Windows11，CPU型号为12th Gen Intel(R) Core(TM) i5-12500H   3.10 GHz，运行内存为32GB，GPU型号为NVIDIA GeForce RTX 3060 Laptop GPU，显存大小为6GB 。本项目的主体网络框架YoloV5基于Pytorch深度学习框架，使用编程语言Python。使用CUDA11.7.102对GPU进行加速。

2.参数设置

> lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0

> 迭代轮次epoch = 80
>
> 批量大小batch size = 8
>
> 学习率learning rate = 0.01
>
> 动量Momentum  = 0.937
>
> 权重衰减weight_decay=0.0005

3.评价指标

> 本项目使用map@0.5，模型参数量，回归率，模型权重大小，每帧推理时间，F1-SCORE作为目标检测模型的衡量标准。其中，MAP@0.5表示在IOU阈值为0.5时的平均AP，模型参数量和模型权重大小衡量了模型的复杂程度。
>
> AP计算公式如下：
>
> ![image-20240229140915405](./../photo/image-20240229140915405.png)
>
> 上述公式中，TP表示IOU大于指定阈值的目标数量，FP表示IOU小于指定阈值的目标数量，FN表示漏检的目标数量。
>
> F1-score计算公式如下：
>
> <img src="./../photo/image-20240229141223796.png" alt="image-20240229141223796" style="zoom:67%;" />

3.实验结果对比

> yolov5模型以及改进过后的YOLOv5m-BiFPN-SE模型在训练中使用了完全一样的数据集以及参数配置，下图为两者在训练中的Loss变化曲线对比图：
>
> <img src="./../photo/image-20240229154632505.png" alt="image-20240229154632505" style="zoom:33%;" />
>
> <img src="./../photo/image-20240229154649558.png" alt="image-20240229154649558" style="zoom:33%;" />
>
> 分析上图结果可知，改进后的模型下相比yolov5m具备更快的收敛速度以及更低的Loss。
>
> | 算法            | 权重/MB | 模型参数量 | MAP@0.5 | MAP@0.5:0.95 | 每帧推理时间/ms |
> | --------------- | ------- | ---------- | ------- | ------------ | --------------- |
> | YOLOv5m         | 40.11   | 20852934   | 0.977   | 0.811        | 24.4            |
> | YOLOv5-BiFPN-SE | 40.56   | 21098319   | 0.980   | 0.821        | 24.5            |
>
> 分析上表可知，改进后的模型虽然参数量比原模型更大，但是实际推理时间两者相差仅有0.1ms。对比mAP@0.5指标，改进后的模型也有一定的提升，对于更加严格的mAP@0.5:0.95指标，改进后的YOLOv5-BiFPN-SE模型有着更大的提升。改进后的模型虽然模型复杂度稍大，但在精度上有着良好的提升。



4.消融实验

> 进行消融实验验证改进注意力机制以及特征金字塔对于网络整体的优化效果，实验结果如下表所示：
>
> 优化模型1在backbone中加入了SE注意力机制，优化模型2修改了网络的特征金字塔结构。由下表可知，加入注意力机制后，虽然每帧的推理时间有稍微延长，但mAP@0.5以及mAP@0.5：0.95都存在一定的提升。同样的，修改特征金字塔之后，虽然每帧的推理时间有稍微延长，但mAP@0.5以及mAP@0.5：0.95都存在一定的提升。

| 模型             | 注意力机制添加 | 特征金字塔修改 | MAP@0.5 | MAP@0.5:0.95 | 每帧推理时间/ms |
| ---------------- | -------------- | -------------- | ------- | ------------ | --------------- |
| YOLOv5m          | 无             | 无             | 0.977   | 0.811        | 24.4            |
| 优化模型1        | 有             | 无             | 0.979   | 0.818        | 25.3            |
| 优化模型2        | 无             | 有             | 0.978   | 0.817        | 24.6            |
| YOLOv5m-BiFPN-SE | 有             | 有             | 0.980   | 0.821        | 24.5            |

5.检测结果分析

> 为了验证改进后YOLOv5m-BiFPN-SE模型的泛化能力，我们在测试集中选取了具备代表性背景特征的图像进行测试，包含理想光照条件、几乎无光照条件、过曝以及局部遮挡的情况，结果如下图所示，可见模型对于多种场景都具备较好的识别性能，具有良好的鲁棒性。
>
> <img src="./../photo/image-20240229164048260.png" alt="image-20240229164048260" style="zoom: 25%;" />
>
> <img src="./../photo/image-20240229164110725.png" alt="image-20240229164110725" style="zoom:25%;" />
>
> <img src="./../photo/image-20240229164124311.png" alt="image-20240229164124311" style="zoom:25%;" />
>
> <img src="./../photo/image-20240229164138651.png" alt="image-20240229164138651" style="zoom:25%;" />

